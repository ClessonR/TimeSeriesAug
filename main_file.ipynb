{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# example of fitting an auxiliary classifier gan (ac-gan) on fashion mnsit\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import expand_dims\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras.datasets.fashion_mnist import load_data\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "from keras.initializers import RandomNormal\n",
    "from matplotlib import pyplot\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "import os\n",
    "#from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Data():\n",
    "    def __init__(self, split, subject,  dataset,path):\n",
    "        \n",
    "        assert split in ['train', 'test']\n",
    "        \n",
    "        self.dataset_root = path\n",
    "        self.dataset_path = os.path.join(self.dataset_root, dataset)\n",
    "        self.subject = subject        \n",
    "        self.split = split\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "        \n",
    "\n",
    "\n",
    "    def load(self):\n",
    "\n",
    "        self.x = np.load(self.dataset_path+f\"/Subject_{self.subject}/\"+'X_train.npy', allow_pickle=True)\n",
    "\n",
    "        if self.split == 'train':\n",
    "            self.y = np.load(self.dataset_path+f\"/Subject_{self.subject}/\"+'y_train.npy', allow_pickle=True)\n",
    "        else:\n",
    "            self.y = np.load(self.dataset_path+f\"/Subject_{self.subject}/\"+'y_test.npy', allow_pickle=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return self.x, self.y\n",
    "    \n",
    "    \n",
    "def y_translator(y,subject,dataset):\n",
    "    # Define your letters\n",
    "    letters = set(y)\n",
    "    y_train_original = y\n",
    "    # Create a dictionary to map letters to numbers\n",
    "    letter_to_number = {letter: i for i, letter in enumerate(letters)}\n",
    "\n",
    "    path = f\"{dataset}/dictionary/\"\n",
    "\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    np.save(f'{path}Subject_{subject}.npy', letter_to_number) \n",
    "\n",
    "    # Test the conversion\n",
    "    y_train_new=[]\n",
    "    for letter in y_train_original:\n",
    "        input_letter = letter\n",
    "        if input_letter in letter_to_number:\n",
    "                y_train_new.append(letter_to_number[input_letter])\n",
    "            #print(f\"The number corresponding to {input_letter} is {letter_to_number[input_letter]}\")\n",
    "        else:\n",
    "            print(f\"{input_letter} is not a valid letter.\")\n",
    "    return(np.array(y_train_new))\n",
    "\n",
    "\n",
    "def load_real_samples(split,subject,dataset):\n",
    "\n",
    "    test = Data(split,subject=subject,dataset=dataset,path=f'')\n",
    "\n",
    "    X,y = test.load()\n",
    "    y = y_translator(y,subject,dataset)\n",
    "    y = y.reshape(-1, 1)\n",
    "    return [X,y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_real_samples(\"train\",subject='02',dataset=\"FALL-mag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[0]\n",
    "y = dataset[1] # y apparently only have 3 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27017, 50, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27017, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So, I have a X with shape (27017, 50, 3) and y with shape (27017, 1).\n",
    "# I need a way to augment that through hugging face API.\n",
    "# Then I need another time series classificator and test the performance on the augmented data and the not augmented data. Then collect the results.\n",
    "# That is pretty much it, goodnight me. See ya tomorrow again.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I need to transform the data into a NLP format through Hugging Face lib's. Then, I'll need to adapt Hugging face augmentation to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/43.8 kB ? eta -:--:--\n",
      "     -------------------------- ----------- 30.7/43.8 kB 262.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 43.8/43.8 kB 308.0 kB/s eta 0:00:00\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tsaug\n",
      "  Downloading tsaug-0.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (1.2.2)\n",
      "Collecting nlpaug\n",
      "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.5.15-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-16.1.0-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: scipy>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tsaug) (1.13.0)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting gdown>=4.0.0 (from nlpaug)\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Downloading transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/9.1 MB 1.9 MB/s eta 0:00:05\n",
      "    --------------------------------------- 0.2/9.1 MB 1.5 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.3/9.1 MB 1.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.5/9.1 MB 2.9 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.9/9.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.1/9.1 MB 3.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.0/9.1 MB 6.2 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 2.3/9.1 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.2/9.1 MB 10.0 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 5.1/9.1 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.2/9.1 MB 13.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.1/9.1 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 15.7 MB/s eta 0:00:00\n",
      "Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "   ---------------------------------------- 0.0/542.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 542.0/542.0 kB 35.5 MB/s eta 0:00:00\n",
      "Downloading tsaug-0.2.1-py3-none-any.whl (32 kB)\n",
      "Downloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "   ---------------------------------------- 0.0/410.5 kB ? eta -:--:--\n",
      "   --------------------------------------- 410.5/410.5 kB 25.0 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 172.0/172.0 kB 10.1 MB/s eta 0:00:00\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "   ---------------------------------------- 0.0/401.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 401.7/401.7 kB 12.6 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 301.8/301.8 kB 18.2 MB/s eta 0:00:00\n",
      "Downloading pyarrow-16.1.0-cp310-cp310-win_amd64.whl (25.9 MB)\n",
      "   ---------------------------------------- 0.0/25.9 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.9/25.9 MB 92.8 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 4.9/25.9 MB 62.8 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.2/25.9 MB 57.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.5/25.9 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 12.2/25.9 MB 50.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.6/25.9 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.8/25.9 MB 59.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 21.8/25.9 MB 59.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.8/25.9 MB 65.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.9/25.9 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.9/25.9 MB 46.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.5.15-cp310-cp310-win_amd64.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/269.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.0/269.0 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
      "   ---------------------------------------- 0.0/287.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 287.4/287.4 kB 18.5 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ---------------------------------------  2.2/2.2 MB 46.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 35.6 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "   ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 134.8/134.8 kB ? eta 0:00:00\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Installing collected packages: xxhash, threadpoolctl, safetensors, regex, pyarrow-hotfix, pyarrow, joblib, fsspec, dill, tsaug, multiprocess, huggingface-hub, tokenizers, gdown, transformers, nlpaug, datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'C:\\\\Users\\\\PC\\\\AppData\\\\Roaming\\\\Python\\\\Python310\\\\site-packages\\\\safetensors\\\\tensorflow.py'\n",
      "Check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in c:\\programdata\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.19.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tsaug\n",
      "  Using cached tsaug-0.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (1.2.2)\n",
      "Collecting nlpaug\n",
      "  Using cached nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (from torch) (4.12.0)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp310-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Using cached pyarrow-16.1.0-cp310-cp310-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Using cached pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp310-cp310-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.3.1,>=2023.1.0 (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: scipy>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tsaug) (1.13.0)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting gdown>=4.0.0 (from nlpaug)\n",
      "  Using cached gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from gdown>=4.0.0->nlpaug) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\pc\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.5)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n",
      "Using cached transformers-4.41.1-py3-none-any.whl (9.1 MB)\n",
      "Using cached datasets-2.19.1-py3-none-any.whl (542 kB)\n",
      "Using cached tsaug-0.2.1-py3-none-any.whl (32 kB)\n",
      "Using cached nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Using cached huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached pyarrow-16.1.0-cp310-cp310-win_amd64.whl (25.9 MB)\n",
      "Using cached regex-2024.5.15-cp310-cp310-win_amd64.whl (268 kB)\n",
      "Using cached safetensors-0.4.3-cp310-none-win_amd64.whl (287 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-none-win_amd64.whl (2.2 MB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Using cached xxhash-3.4.1-cp310-cp310-win_amd64.whl (29 kB)\n",
      "Installing collected packages: xxhash, threadpoolctl, safetensors, regex, pyarrow-hotfix, pyarrow, joblib, fsspec, dill, tsaug, multiprocess, huggingface-hub, tokenizers, gdown, transformers, nlpaug, datasets\n",
      "Successfully installed datasets-2.19.1 dill-0.3.8 fsspec-2024.3.1 gdown-5.2.0 huggingface-hub-0.23.2 joblib-1.4.2 multiprocess-0.70.16 nlpaug-1.1.11 pyarrow-16.1.0 pyarrow-hotfix-0.6 regex-2024.5.15 safetensors-0.4.3 threadpoolctl-3.5.0 tokenizers-0.19.1 transformers-4.41.1 tsaug-0.2.1 xxhash-3.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script gdown.exe is installed in 'C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script datasets-cli.exe is installed in 'C:\\Users\\PC\\AppData\\Roaming\\Python\\Python310\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "shap 0.42.1 requires cloudpickle, which is not installed.\n",
      "tsgm 0.0.4 requires networkx, which is not installed.\n",
      "tsgm 0.0.4 requires seaborn, which is not installed.\n",
      "sktime 0.26.0 requires pandas<2.2.0,>=1.1, but you have pandas 2.2.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers datasets tsaug scikit-learn nlpaug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Dataset' from 'torch.utils.data' (unknown location)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTimeSeriesDataset\u001b[39;00m(Dataset):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'Dataset' from 'torch.utils.data' (unknown location)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.X[idx]\n",
    "        label = self.y[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return torch.tensor(sample, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
